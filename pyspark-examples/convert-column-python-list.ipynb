{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the findspark library to locate spark on our local machine\n",
    "import findspark\n",
    "findspark.init(r'C:\\spark\\spark-3.5.0-bin-hadoop3')\n",
    "import pyspark # only run this after findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n",
    "  ]\n",
    "columns=[\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "print(df.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "df.rdd: It assumes that df is a DataFrame in PySpark. This line converts the DataFrame df into an RDD (Resilient Distributed Dataset). RDD is a fundamental data structure in Spark, which represents a distributed collection of data that can be processed in parallel across a cluster of machines.\n",
    "\n",
    ".map(lambda x: x[3]): This line applies a map transformation on the RDD. The map transformation takes a lambda function (anonymous function) as an argument and applies it to each element of the RDD. In this case, the lambda function lambda x: x[3] is applied to each row x in the RDD, and it selects the value at index 3 of each row. This assumes that the rows in your DataFrame df are structured in such a way that you can access elements by index.\n",
    "\n",
    ".collect(): Finally, the collect action is called on the RDD. The collect action retrieves all the elements that resulted from the map transformation and returns them as a local Python list. This means that all the values from the 4th column (0-based indexing) of the DataFrame are collected into a Python list named states1.\n",
    "\n",
    "After executing this line of code, states1 will contain all the values from the 4th column of your DataFrame as a list in the local Python environment. Keep in mind that using collect() can be inefficient and should be avoided for large RDDs or DataFrames because it brings all the data from the distributed Spark cluster to the local machine, potentially causing performance and memory issues. It's better to use Spark's distributed operations whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CA', 'NY', 'CA', 'FL']\n"
     ]
    }
   ],
   "source": [
    "states1=df.rdd.map(lambda x: x[3]).collect()\n",
    "print(states1)\n",
    "\n",
    "#['CA', 'NY', 'CA', 'FL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CA', 'NY', 'FL']\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict \n",
    "res = list(OrderedDict.fromkeys(states1)) \n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
