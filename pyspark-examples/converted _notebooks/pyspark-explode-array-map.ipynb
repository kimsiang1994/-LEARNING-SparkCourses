{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-------------------+--------------------+\n",
      "|      name|     knownLanguages|          properties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, NULL]|{eye -> NULL, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               NULL|                NULL|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we use the findspark library to locate spark on our local machine\n",
    "import findspark\n",
    "findspark.init(r'C:\\spark\\spark-3.5.0-bin-hadoop3')\n",
    "import pyspark # only run this after findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
    "\n",
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})\n",
    "        ]\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  NULL|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name,explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-------+----+-----+\n",
      "|   name| key|value|\n",
      "+-------+----+-----+\n",
      "|  James| eye|brown|\n",
      "|  James|hair|black|\n",
      "|Michael| eye| NULL|\n",
      "|Michael|hair|brown|\n",
      "| Robert| eye|     |\n",
      "| Robert|hair|  red|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df3 = df.select(df.name,explode(df.properties))\n",
    "df3.printSchema()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.select(...): This selects one or more columns from a DataFrame (df). It returns a new DataFrame containing only the specified columns.\n",
    "\n",
    "df.name: This selects the column named \"name\" from the DataFrame df.\n",
    "\n",
    "explode_outer(df.knownLanguages): This is a PySpark function called explode_outer. It is used to transform a column of arrays or maps into multiple rows, with each row containing a single element from the array or map. In this case, it is applied to the column knownLanguages.\n",
    "\n",
    ".show(): This is used to display the resulting DataFrame on the console.\n",
    "\n",
    "Putting it all together, the code takes a DataFrame df, selects the \"name\" column, and then applies the explode_outer function to the \"knownLanguages\" column. This will generate a new DataFrame where each row corresponds to a single language known by a person, and if a person knows multiple languages, there will be multiple rows for that person with the same name but different languages. The result is displayed on the console using the show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      name|   col|\n",
      "+----------+------+\n",
      "|     James|  Java|\n",
      "|     James| Scala|\n",
      "|   Michael| Spark|\n",
      "|   Michael|  Java|\n",
      "|   Michael|  NULL|\n",
      "|    Robert|CSharp|\n",
      "|    Robert|      |\n",
      "|Washington|  NULL|\n",
      "| Jefferson|     1|\n",
      "| Jefferson|     2|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,explode_outer(df.knownLanguages)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+\n",
      "|      name| key|value|\n",
      "+----------+----+-----+\n",
      "|     James| eye|brown|\n",
      "|     James|hair|black|\n",
      "|   Michael| eye| NULL|\n",
      "|   Michael|hair|brown|\n",
      "|    Robert| eye|     |\n",
      "|    Robert|hair|  red|\n",
      "|Washington|NULL| NULL|\n",
      "| Jefferson|NULL| NULL|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,explode_outer(df.properties)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.select(...): This selects one or more columns from a DataFrame (df). It returns a new DataFrame containing only the specified columns.\n",
    "\n",
    "df.name: This selects the column named \"name\" from the DataFrame df.\n",
    "\n",
    "posexplode(df.knownLanguages): This is a PySpark function called posexplode. It is used to transform a column of arrays into multiple rows, with each row containing the index (position) and the corresponding element from the array. In this case, it is applied to the column knownLanguages.\n",
    "\n",
    ".show(): This is used to display the resulting DataFrame on the console.\n",
    "\n",
    "Putting it all together, the code takes a DataFrame df, selects the \"name\" column, and then applies the posexplode function to the \"knownLanguages\" column. This will generate a new DataFrame where each row corresponds to a single element (language) from the \"knownLanguages\" array along with its position (index) in the array. The result is displayed on the console using the show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------+\n",
      "|     name|pos|   col|\n",
      "+---------+---+------+\n",
      "|    James|  0|  Java|\n",
      "|    James|  1| Scala|\n",
      "|  Michael|  0| Spark|\n",
      "|  Michael|  1|  Java|\n",
      "|  Michael|  2|  NULL|\n",
      "|   Robert|  0|CSharp|\n",
      "|   Robert|  1|      |\n",
      "|Jefferson|  0|     1|\n",
      "|Jefferson|  1|     2|\n",
      "+---------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import posexplode\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,posexplode(df.knownLanguages)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----+-----+\n",
      "|   name|pos| key|value|\n",
      "+-------+---+----+-----+\n",
      "|  James|  0| eye|brown|\n",
      "|  James|  1|hair|black|\n",
      "|Michael|  0| eye| NULL|\n",
      "|Michael|  1|hair|brown|\n",
      "| Robert|  0| eye|     |\n",
      "| Robert|  1|hair|  red|\n",
      "+-------+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,posexplode(df.properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+\n",
      "|      name| pos|   col|\n",
      "+----------+----+------+\n",
      "|     James|   0|  Java|\n",
      "|     James|   1| Scala|\n",
      "|   Michael|   0| Spark|\n",
      "|   Michael|   1|  Java|\n",
      "|   Michael|   2|  NULL|\n",
      "|    Robert|   0|CSharp|\n",
      "|    Robert|   1|      |\n",
      "|Washington|NULL|  NULL|\n",
      "| Jefferson|   0|     1|\n",
      "| Jefferson|   1|     2|\n",
      "+----------+----+------+\n",
      "\n",
      "+----------+----+----+-----+\n",
      "|      name| pos| key|value|\n",
      "+----------+----+----+-----+\n",
      "|     James|   0| eye|brown|\n",
      "|     James|   1|hair|black|\n",
      "|   Michael|   0| eye| NULL|\n",
      "|   Michael|   1|hair|brown|\n",
      "|    Robert|   0| eye|     |\n",
      "|    Robert|   1|hair|  red|\n",
      "|Washington|NULL|NULL| NULL|\n",
      "| Jefferson|NULL|NULL| NULL|\n",
      "+----------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import posexplode_outer\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,posexplode_outer(df.knownLanguages)).show()\n",
    "\n",
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,posexplode_outer(df.properties)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
