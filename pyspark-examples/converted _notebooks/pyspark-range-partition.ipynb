{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|   10|\n",
      "|  2|   20|\n",
      "|  3|   10|\n",
      "|  4|   20|\n",
      "|  5|   10|\n",
      "|  6|   30|\n",
      "|  7|   50|\n",
      "|  8|   50|\n",
      "|  9|   50|\n",
      "| 10|   30|\n",
      "| 11|   10|\n",
      "| 12|   10|\n",
      "| 13|   40|\n",
      "| 14|   40|\n",
      "| 15|   40|\n",
      "| 16|   40|\n",
      "| 17|   50|\n",
      "| 18|   10|\n",
      "| 19|   40|\n",
      "| 20|   40|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we use the findspark library to locate spark on our local machine\n",
    "import findspark\n",
    "findspark.init(r'C:\\spark\\spark-3.5.0-bin-hadoop3')\n",
    "import pyspark # only run this after findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName('SparkByExamples.com') \\\n",
    "          .getOrCreate()\n",
    "          \n",
    "\n",
    "data = [(1,10),(2,20),(3,10),(4,20),(5,10),\n",
    "    (6,30),(7,50),(8,50),(9,50),(10,30),\n",
    "    (11,10),(12,10),(13,40),(14,40),(15,40),\n",
    "    (16,40),(17,50),(18,10),(19,40),(20,40)\n",
    "  ]\n",
    "\n",
    "df=spark.createDataFrame(data,[\"id\",\"value\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df is assumed to be a DataFrame that you want to perform some operation on.\n",
    "\n",
    "repartition(3, \"value\") is used to repartition the DataFrame df into three partitions based on the \"value\" column. This operation reshuffles the data in the DataFrame and redistributes it into the specified number of partitions. In this case, you are requesting three partitions, and the data is being partitioned based on the values in the \"value\" column.\n",
    "\n",
    ".explain(True) is called on the resulting DataFrame after the repartitioning. The explain() method is used to display the execution plan of the DataFrame operation. The True argument passed to explain() indicates that you want to display a detailed (extended) explanation of the execution plan, including physical and logical plans.\n",
    "\n",
    "When you run this code, it will print out the detailed execution plan for the repartitioned DataFrame, showing how Spark plans to perform the repartitioning operation and any other related operations. This can be helpful for understanding the underlying optimizations and transformations that Spark performs to execute your DataFrame operations efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'RepartitionByExpression ['value], 3\n",
      "+- LogicalRDD [id#26L, value#27L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, value: bigint\n",
      "RepartitionByExpression [value#27L], 3\n",
      "+- LogicalRDD [id#26L, value#27L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "RepartitionByExpression [value#27L], 3\n",
      "+- LogicalRDD [id#26L, value#27L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange hashpartitioning(value#27L, 3), REPARTITION_BY_NUM, [plan_id=42]\n",
      "   +- Scan ExistingRDD[id#26L,value#27L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.repartition(3,\"value\").explain(True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repartitionByRange(\"value\") is used to repartition the DataFrame based on the values in the \"value\" column using range-based partitioning. Range-based partitioning distributes the data into partitions based on specified ranges of values within the column. Each partition will contain a specific range of values from the \"value\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'RepartitionByExpression ['value ASC NULLS FIRST]\n",
      "+- LogicalRDD [id#26L, value#27L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, value: bigint\n",
      "RepartitionByExpression [value#27L ASC NULLS FIRST]\n",
      "+- LogicalRDD [id#26L, value#27L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "RepartitionByExpression [value#27L ASC NULLS FIRST]\n",
      "+- LogicalRDD [id#26L, value#27L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange rangepartitioning(value#27L ASC NULLS FIRST, 200), REPARTITION_BY_COL, [plan_id=124]\n",
      "   +- Scan ExistingRDD[id#26L,value#27L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.repartitionByRange(\"value\").explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartitionByRange(3,\"value\").explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
