{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use the findspark library to locate spark on our local machine\n",
    "import findspark\n",
    "findspark.init(r'C:\\spark\\spark-3.5.0-bin-hadoop3')\n",
    "import pyspark # only run this after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, ArrayType,StructType,StructField\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- languagesAtWork: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      " |-- previousState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n",
      "+----------------+------------------+---------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n",
    " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n",
    " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n",
    "]\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"name\",StringType(),True), \n",
    "    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n",
    "    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n",
    "    StructField(\"currentState\", StringType(), True), \n",
    "    StructField(\"previousState\", StringType(), True) \n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.select(df.name, explode(df.languagesAtSchool)):\n",
    "\n",
    "This part of the code selects two columns from the DataFrame df: the \"name\" column and a new column generated by applying the explode function to the \"languagesAtSchool\" column.\n",
    "The explode function is used to \"explode\" an array or a map column into multiple rows, with each row containing one element from the array or map.\n",
    "In this case, explode(df.languagesAtSchool) takes the \"languagesAtSchool\" column, which is assumed to be an array or a list of values, and generates a new DataFrame where each row corresponds to a single element from the array, and the \"name\" column is duplicated for each element.\n",
    ".show(): Finally, the show method is called on the resulting DataFrame to display its contents in the console.\n",
    "\n",
    "So, the overall purpose of this code is to explode the \"languagesAtSchool\" array column into multiple rows, with each row containing one language from the array. The \"name\" column remains the same for each row, indicating the association of each language with a particular name from the original DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|            name|   col|\n",
      "+----------------+------+\n",
      "|    James,,Smith|  Java|\n",
      "|    James,,Smith| Scala|\n",
      "|    James,,Smith|   C++|\n",
      "|   Michael,Rose,| Spark|\n",
      "|   Michael,Rose,|  Java|\n",
      "|   Michael,Rose,|   C++|\n",
      "|Robert,,Williams|CSharp|\n",
      "|Robert,,Williams|    VB|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(df.name,explode(df.languagesAtSchool)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.select(split(df.name, \",\").alias(\"nameAsArray\")):\n",
    "\n",
    "This code selects the \"name\" column from the DataFrame df and applies the split function to it.\n",
    "The split function is used to split a string column into an array of strings based on a specified delimiter, in this case, a comma (,).\n",
    "The result of the split function is given the alias \"nameAsArray,\" which means the resulting array will be stored in a new column called \"nameAsArray.\"\n",
    ".show(): Finally, the show method is called on the resulting DataFrame to display its contents in the console.\n",
    "\n",
    "So, the overall purpose of this code is to take the \"name\" column, which contains comma-separated values, and split it into an array of strings. Each element of the resulting array represents a part of the original string that was separated by commas. This can be useful for breaking down a single column with delimited values into multiple columns or for further processing the data as arrays in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         nameAsArray|\n",
      "+--------------------+\n",
      "|    [James, , Smith]|\n",
      "|   [Michael, Rose, ]|\n",
      "|[Robert, , Williams]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df.select(split(df.name,\",\").alias(\"nameAsArray\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|            name|  States|\n",
      "+----------------+--------+\n",
      "|    James,,Smith|[OH, CA]|\n",
      "|   Michael,Rose,|[NY, NJ]|\n",
      "|Robert,,Williams|[UT, NV]|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array\n",
    "df.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|            name|array_contains|\n",
      "+----------------+--------------+\n",
      "|    James,,Smith|          true|\n",
      "|   Michael,Rose,|          true|\n",
      "|Robert,,Williams|         false|\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "df.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n",
    "    .alias(\"array_contains\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
