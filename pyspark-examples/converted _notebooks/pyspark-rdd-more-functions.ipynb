{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Alice’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n"
     ]
    }
   ],
   "source": [
    "#we use the findspark library to locate spark on our local machine\n",
    "import findspark\n",
    "findspark.init(r'C:\\spark\\spark-3.5.0-bin-hadoop3')\n",
    "import pyspark # only run this after findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [\"Project\",\n",
    "\"Gutenberg’s\",\n",
    "\"Alice’s\",\n",
    "\"Adventures\",\n",
    "\"in\",\n",
    "\"Wonderland\",\n",
    "\"Project\",\n",
    "\"Gutenberg’s\",\n",
    "\"Adventures\",\n",
    "\"in\",\n",
    "\"Wonderland\",\n",
    "\"Project\",\n",
    "\"Gutenberg’s\"]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd2=rdd.map(lambda x: (x,1))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|    30|\n",
      "|     Anna|    Rose|     F|    41|\n",
      "|   Robert|Williams|     M|    62|\n",
      "+---------+--------+------+------+\n",
      "\n",
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|        60|\n",
      "|      Anna,Rose|     F|        82|\n",
      "|Robert,Williams|     M|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','Smith','M',30),\n",
    "  ('Anna','Rose','F',41),\n",
    "  ('Robert','Williams','M',62), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()\n",
    "\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x[0]+\",\"+x[1],x[2],x[3]*2)\n",
    "    )  \n",
    "df2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+---+\n",
      "|             _1| _2| _3|\n",
      "+---------------+---+---+\n",
      "|    James,Smith|  m| 60|\n",
      "|      Anna,Rose|  f| 82|\n",
      "|Robert,Williams|  m|124|\n",
      "+---------------+---+---+\n",
      "\n",
      "+---------------+---+---+\n",
      "|             _1| _2| _3|\n",
      "+---------------+---+---+\n",
      "|    James,Smith|  m| 60|\n",
      "|      Anna,Rose|  f| 82|\n",
      "|Robert,Williams|  m|124|\n",
      "+---------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Referring Column Names\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n",
    "    ) \n",
    "\n",
    "\n",
    "#Referring Column Names\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x.firstname+\",\"+x.lastname,x.gender,x.salary*2)\n",
    "    ) \n",
    "\n",
    "\n",
    "def func1(x):\n",
    "    firstName=x.firstname\n",
    "    lastName=x.lastname\n",
    "    name=firstName+\",\"+lastName\n",
    "    gender=x.gender.lower()\n",
    "    salary=x.salary*2\n",
    "    return (name,gender,salary)\n",
    "\n",
    "rdd2=df.rdd.map(lambda x: func1(x)).toDF().show()\n",
    "rdd2=df.rdd.map(func1).toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rdd is assumed to be an RDD containing key-value pairs, typically represented as tuples.\n",
    "\n",
    "reduceByKey is a transformation operation that groups the data in the RDD by keys and then applies a reduce function to aggregate values for each key.\n",
    "\n",
    "In this case, the lambda function lambda a, b: a + b is used as the reduce function. This function takes two values a and b and returns their sum. It is applied to all values with the same key.\n",
    "\n",
    "The reduceByKey operation groups the data by keys and then applies the provided reduce function to the values associated with each key, resulting in a new RDD where each key is associated with a single reduced value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Alice’s', 1)\n",
      "('Gutenberg’s', 3)\n",
      "('in', 2)\n",
      "('Wonderland', 2)\n",
      "('Project', 3)\n",
      "('Adventures', 2)\n"
     ]
    }
   ],
   "source": [
    "data = [('Project', 1),\n",
    "('Gutenberg’s', 1),\n",
    "('Alice’s', 1),\n",
    "('Adventures', 1),\n",
    "('in', 1),\n",
    "('Wonderland', 1),\n",
    "('Project', 1),\n",
    "('Gutenberg’s', 1),\n",
    "('Adventures', 1),\n",
    "('in', 1),\n",
    "('Wonderland', 1),\n",
    "('Project', 1),\n",
    "('Gutenberg’s', 1)]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd2=rdd.reduceByKey(lambda a,b: a+b)\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|_1       |_2 |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \n",
    "        (\"Marketing\",20), \n",
    "        (\"Sales\",30), \n",
    "        (\"IT\",40) \n",
    "      ]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "\n",
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "deptSchema = StructType([       \n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(data=dept, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s\n",
      "Alice’s Adventures in Wonderland\n",
      "Project Gutenberg’s\n",
      "Adventures in Wonderland\n",
      "Project Gutenberg’s\n",
      "Project\n",
      "Gutenberg’s\n",
      "Alice’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Alice’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Alice’s', 1)\n",
      "('Gutenberg’s', 3)\n",
      "('in', 2)\n",
      "('Wonderland', 2)\n",
      "('Project', 3)\n",
      "('Adventures', 2)\n",
      "(1, 'Alice’s')\n",
      "(2, 'in')\n",
      "(2, 'Wonderland')\n",
      "(2, 'Adventures')\n",
      "(3, 'Gutenberg’s')\n",
      "(3, 'Project')\n",
      "(2, 'Wonderland')\n",
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "\n",
    "for element in rdd.collect():\n",
    "    print(element)\n",
    "\n",
    "#Flatmap    \n",
    "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
    "for element in rdd2.collect():\n",
    "    print(element)\n",
    "#map\n",
    "rdd3=rdd2.map(lambda x: (x,1))\n",
    "for element in rdd3.collect():\n",
    "    print(element)\n",
    "#reduceByKey\n",
    "rdd4=rdd3.reduceByKey(lambda a,b: a+b)\n",
    "for element in rdd4.collect():\n",
    "    print(element)\n",
    "#map\n",
    "rdd5 = rdd4.map(lambda x: (x[1],x[0])).sortByKey()\n",
    "for element in rdd5.collect():\n",
    "    print(element)\n",
    "#filter\n",
    "rdd6 = rdd5.filter(lambda x : 'a' in x[1])\n",
    "for element in rdd6.collect():\n",
    "    print(element)\n",
    "\n",
    "from pyspark.sql.functions import col,expr\n",
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n",
    "spark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n",
    "    .select(col(\"date\"),col(\"increment\"), \\\n",
    "      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
